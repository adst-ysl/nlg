{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjB9UB4Qf8Yq",
        "colab_type": "text"
      },
      "source": [
        "## start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN3pPGzdQesl",
        "colab_type": "code",
        "outputId": "160f2081-f5f8-43f9-9676-69259f8408aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mount')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/mount\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQvf2Qopk2W",
        "colab_type": "code",
        "outputId": "1b4f20e2-2dc2-4fcd-f2ce-31b1cf56fe72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "print(os.environ['PYTHONPATH'] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc9TFvY7upVV",
        "colab_type": "code",
        "outputId": "2e7fe3a2-00e7-4a01-bd71-a019b83a71a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8HVOHM-OW4X",
        "colab_type": "text"
      },
      "source": [
        "##dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmg6ttVfM48R",
        "colab_type": "code",
        "outputId": "2e245f87-5901-4af0-e6e7-a2e5d30dc022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!git clone  https://github.com/yuslzq/transformer.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformer'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/138)\u001b[K\rremote: Counting objects:   1% (2/138)\u001b[K\rremote: Counting objects:   2% (3/138)\u001b[K\rremote: Counting objects:   3% (5/138)\u001b[K\rremote: Counting objects:   4% (6/138)\u001b[K\rremote: Counting objects:   5% (7/138)\u001b[K\rremote: Counting objects:   6% (9/138)\u001b[K\rremote: Counting objects:   7% (10/138)\u001b[K\rremote: Counting objects:   8% (12/138)\u001b[K\rremote: Counting objects:   9% (13/138)\u001b[K\rremote: Counting objects:  10% (14/138)\u001b[K\rremote: Counting objects:  11% (16/138)\u001b[K\rremote: Counting objects:  12% (17/138)\u001b[K\rremote: Counting objects:  13% (18/138)\u001b[K\rremote: Counting objects:  14% (20/138)\u001b[K\rremote: Counting objects:  15% (21/138)\u001b[K\rremote: Counting objects:  16% (23/138)\u001b[K\rremote: Counting objects:  17% (24/138)\u001b[K\rremote: Counting objects:  18% (25/138)\u001b[K\rremote: Counting objects:  19% (27/138)\u001b[K\rremote: Counting objects:  20% (28/138)\u001b[K\rremote: Counting objects:  21% (29/138)\u001b[K\rremote: Counting objects:  22% (31/138)\u001b[K\rremote: Counting objects:  23% (32/138)\u001b[K\rremote: Counting objects:  24% (34/138)\u001b[K\rremote: Counting objects:  25% (35/138)\u001b[K\rremote: Counting objects:  26% (36/138)\u001b[K\rremote: Counting objects:  27% (38/138)\u001b[K\rremote: Counting objects:  28% (39/138)\u001b[K\rremote: Counting objects:  29% (41/138)\u001b[K\rremote: Counting objects:  30% (42/138)\u001b[K\rremote: Counting objects:  31% (43/138)\u001b[K\rremote: Counting objects:  32% (45/138)\u001b[K\rremote: Counting objects:  33% (46/138)\u001b[K\rremote: Counting objects:  34% (47/138)\u001b[K\rremote: Counting objects:  35% (49/138)\u001b[K\rremote: Counting objects:  36% (50/138)\u001b[K\rremote: Counting objects:  37% (52/138)\u001b[K\rremote: Counting objects:  38% (53/138)\u001b[K\rremote: Counting objects:  39% (54/138)\u001b[K\rremote: Counting objects:  40% (56/138)\u001b[K\rremote: Counting objects:  41% (57/138)\u001b[K\rremote: Counting objects:  42% (58/138)\u001b[K\rremote: Counting objects:  43% (60/138)\u001b[K\rremote: Counting objects:  44% (61/138)\u001b[K\rremote: Counting objects:  45% (63/138)\u001b[K\rremote: Counting objects:  46% (64/138)\u001b[K\rremote: Counting objects:  47% (65/138)\u001b[K\rremote: Counting objects:  48% (67/138)\u001b[K\rremote: Counting objects:  49% (68/138)\u001b[K\rremote: Counting objects:  50% (69/138)\u001b[K\rremote: Counting objects:  51% (71/138)\u001b[K\rremote: Counting objects:  52% (72/138)\u001b[K\rremote: Counting objects:  53% (74/138)\u001b[K\rremote: Counting objects:  54% (75/138)\u001b[K\rremote: Counting objects:  55% (76/138)\u001b[K\rremote: Counting objects:  56% (78/138)\u001b[K\rremote: Counting objects:  57% (79/138)\u001b[K\rremote: Counting objects:  58% (81/138)\u001b[K\rremote: Counting objects:  59% (82/138)\u001b[K\rremote: Counting objects:  60% (83/138)\u001b[K\rremote: Counting objects:  61% (85/138)\u001b[K\rremote: Counting objects:  62% (86/138)\u001b[K\rremote: Counting objects:  63% (87/138)\u001b[K\rremote: Counting objects:  64% (89/138)\u001b[K\rremote: Counting objects:  65% (90/138)\u001b[K\rremote: Counting objects:  66% (92/138)\u001b[K\rremote: Counting objects:  67% (93/138)\u001b[K\rremote: Counting objects:  68% (94/138)\u001b[K\rremote: Counting objects:  69% (96/138)\u001b[K\rremote: Counting objects:  70% (97/138)\u001b[K\rremote: Counting objects:  71% (98/138)\u001b[K\rremote: Counting objects:  72% (100/138)\u001b[K\rremote: Counting objects:  73% (101/138)\u001b[K\rremote: Counting objects:  74% (103/138)\u001b[K\rremote: Counting objects:  75% (104/138)\u001b[K\rremote: Counting objects:  76% (105/138)\u001b[K\rremote: Counting objects:  77% (107/138)\u001b[K\rremote: Counting objects:  78% (108/138)\u001b[K\rremote: Counting objects:  79% (110/138)\u001b[K\rremote: Counting objects:  80% (111/138)\u001b[K\rremote: Counting objects:  81% (112/138)\u001b[K\rremote: Counting objects:  82% (114/138)\u001b[K\rremote: Counting objects:  83% (115/138)\u001b[K\rremote: Counting objects:  84% (116/138)\u001b[K\rremote: Counting objects:  85% (118/138)\u001b[K\rremote: Counting objects:  86% (119/138)\u001b[K\rremote: Counting objects:  87% (121/138)\u001b[K\rremote: Counting objects:  88% (122/138)\u001b[K\rremote: Counting objects:  89% (123/138)\u001b[K\rremote: Counting objects:  90% (125/138)\u001b[K\rremote: Counting objects:  91% (126/138)\u001b[K\rremote: Counting objects:  92% (127/138)\u001b[K\rremote: Counting objects:  93% (129/138)\u001b[K\rremote: Counting objects:  94% (130/138)\u001b[K\rremote: Counting objects:  95% (132/138)\u001b[K\rremote: Counting objects:  96% (133/138)\u001b[K\rremote: Counting objects:  97% (134/138)\u001b[K\rremote: Counting objects:  98% (136/138)\u001b[K\rremote: Counting objects:  99% (137/138)\u001b[K\rremote: Counting objects: 100% (138/138)\u001b[K\rremote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/111)\u001b[K\rremote: Compressing objects:   1% (2/111)\u001b[K\rremote: Compressing objects:   2% (3/111)\u001b[K\rremote: Compressing objects:   3% (4/111)\u001b[K\rremote: Compressing objects:   4% (5/111)\u001b[K\rremote: Compressing objects:   5% (6/111)\u001b[K\rremote: Compressing objects:   6% (7/111)\u001b[K\rremote: Compressing objects:   7% (8/111)\u001b[K\rremote: Compressing objects:   8% (9/111)\u001b[K\rremote: Compressing objects:   9% (10/111)\u001b[K\rremote: Compressing objects:  10% (12/111)\u001b[K\rremote: Compressing objects:  11% (13/111)\u001b[K\rremote: Compressing objects:  12% (14/111)\u001b[K\rremote: Compressing objects:  13% (15/111)\u001b[K\rremote: Compressing objects:  14% (16/111)\u001b[K\rremote: Compressing objects:  15% (17/111)\u001b[K\rremote: Compressing objects:  16% (18/111)\u001b[K\rremote: Compressing objects:  17% (19/111)\u001b[K\rremote: Compressing objects:  18% (20/111)\u001b[K\rremote: Compressing objects:  19% (22/111)\u001b[K\rremote: Compressing objects:  20% (23/111)\u001b[K\rremote: Compressing objects:  21% (24/111)\u001b[K\rremote: Compressing objects:  22% (25/111)\u001b[K\rremote: Compressing objects:  23% (26/111)\u001b[K\rremote: Compressing objects:  24% (27/111)\u001b[K\rremote: Compressing objects:  25% (28/111)\u001b[K\rremote: Compressing objects:  26% (29/111)\u001b[K\rremote: Compressing objects:  27% (30/111)\u001b[K\rremote: Compressing objects:  28% (32/111)\u001b[K\rremote: Compressing objects:  29% (33/111)\u001b[K\rremote: Compressing objects:  30% (34/111)\u001b[K\rremote: Compressing objects:  31% (35/111)\u001b[K\rremote: Compressing objects:  32% (36/111)\u001b[K\rremote: Compressing objects:  33% (37/111)\u001b[K\rremote: Compressing objects:  34% (38/111)\u001b[K\rremote: Compressing objects:  35% (39/111)\u001b[K\rremote: Compressing objects:  36% (40/111)\u001b[K\rremote: Compressing objects:  37% (42/111)\u001b[K\rremote: Compressing objects:  38% (43/111)\u001b[K\rremote: Compressing objects:  39% (44/111)\u001b[K\rremote: Compressing objects:  40% (45/111)\u001b[K\rremote: Compressing objects:  41% (46/111)\u001b[K\rremote: Compressing objects:  42% (47/111)\u001b[K\rremote: Compressing objects:  43% (48/111)\u001b[K\rremote: Compressing objects:  44% (49/111)\u001b[K\rremote: Compressing objects:  45% (50/111)\u001b[K\rremote: Compressing objects:  46% (52/111)\u001b[K\rremote: Compressing objects:  47% (53/111)\u001b[K\rremote: Compressing objects:  48% (54/111)\u001b[K\rremote: Compressing objects:  49% (55/111)\u001b[K\rremote: Compressing objects:  50% (56/111)\u001b[K\rremote: Compressing objects:  51% (57/111)\u001b[K\rremote: Compressing objects:  52% (58/111)\u001b[K\rremote: Compressing objects:  53% (59/111)\u001b[K\rremote: Compressing objects:  54% (60/111)\u001b[K\rremote: Compressing objects:  55% (62/111)\u001b[K\rremote: Compressing objects:  56% (63/111)\u001b[K\rremote: Compressing objects:  57% (64/111)\u001b[K\rremote: Compressing objects:  58% (65/111)\u001b[K\rremote: Compressing objects:  59% (66/111)\u001b[K\rremote: Compressing objects:  60% (67/111)\u001b[K\rremote: Compressing objects:  61% (68/111)\u001b[K\rremote: Compressing objects:  62% (69/111)\u001b[K\rremote: Compressing objects:  63% (70/111)\u001b[K\rremote: Compressing objects:  64% (72/111)\u001b[K\rremote: Compressing objects:  65% (73/111)\rremote: Compressing objects:  66% (74/111)\u001b[K\rremote: Compressing objects:  67% (75/111)\u001b[K\rremote: Compressing objects:  68% (76/111)\u001b[K\rremote: Compressing objects:  69% (77/111)\u001b[K\rremote: Compressing objects:  70% (78/111)\u001b[K\rremote: Compressing objects:  71% (79/111)\u001b[K\rremote: Compressing objects:  72% (80/111)\u001b[K\rremote: Compressing objects:  73% (82/111)\u001b[K\rremote: Compressing objects:  74% (83/111)\u001b[K\rremote: Compressing objects:  75% (84/111)\u001b[K\rremote: Compressing objects:  76% (85/111)\u001b[K\rremote: Compressing objects:  77% (86/111)\u001b[K\rremote: Compressing objects:  78% (87/111)\u001b[K\rremote: Compressing objects:  79% (88/111)\u001b[K\rremote: Compressing objects:  80% (89/111)\u001b[K\rremote: Compressing objects:  81% (90/111)\u001b[K\rremote: Compressing objects:  82% (92/111)\u001b[K\rremote: Compressing objects:  83% (93/111)\u001b[K\rremote: Compressing objects:  84% (94/111)\u001b[K\rremote: Compressing objects:  85% (95/111)\u001b[K\rremote: Compressing objects:  86% (96/111)\u001b[K\rremote: Compressing objects:  87% (97/111)\u001b[K\rremote: Compressing objects:  88% (98/111)\u001b[K\rremote: Compressing objects:  89% (99/111)\u001b[K\rremote: Compressing objects:  90% (100/111)\u001b[K\rremote: Compressing objects:  91% (102/111)\u001b[K\rremote: Compressing objects:  92% (103/111)\u001b[K\rremote: Compressing objects:  93% (104/111)\u001b[K\rremote: Compressing objects:  94% (105/111)\u001b[K\rremote: Compressing objects:  95% (106/111)\u001b[K\rremote: Compressing objects:  96% (107/111)\u001b[K\rremote: Compressing objects:  97% (108/111)\u001b[K\rremote: Compressing objects:  98% (109/111)\u001b[K\rremote: Compressing objects:  99% (110/111)\u001b[K\rremote: Compressing objects: 100% (111/111)\u001b[K\rremote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "Receiving objects:   0% (1/207)   \rReceiving objects:   1% (3/207)   \rReceiving objects:   2% (5/207)   \rReceiving objects:   3% (7/207)   \rReceiving objects:   4% (9/207)   \rReceiving objects:   5% (11/207)   \rReceiving objects:   6% (13/207)   \rReceiving objects:   7% (15/207)   \rReceiving objects:   8% (17/207)   \rReceiving objects:   9% (19/207)   \rReceiving objects:  10% (21/207)   \rReceiving objects:  11% (23/207)   \rReceiving objects:  12% (25/207)   \rReceiving objects:  13% (27/207)   \rReceiving objects:  14% (29/207)   \rReceiving objects:  15% (32/207)   \rReceiving objects:  16% (34/207)   \rReceiving objects:  17% (36/207)   \rReceiving objects:  18% (38/207)   \rReceiving objects:  19% (40/207)   \rReceiving objects:  20% (42/207)   \rReceiving objects:  21% (44/207)   \rReceiving objects:  22% (46/207)   \rReceiving objects:  23% (48/207)   \rReceiving objects:  24% (50/207)   \rReceiving objects:  25% (52/207)   \rReceiving objects:  26% (54/207)   \rReceiving objects:  27% (56/207)   \rReceiving objects:  28% (58/207)   \rReceiving objects:  29% (61/207)   \rReceiving objects:  30% (63/207)   \rReceiving objects:  31% (65/207)   \rReceiving objects:  32% (67/207)   \rReceiving objects:  33% (69/207)   \rReceiving objects:  34% (71/207)   \rReceiving objects:  35% (73/207)   \rReceiving objects:  36% (75/207)   \rReceiving objects:  37% (77/207)   \rReceiving objects:  38% (79/207)   \rReceiving objects:  39% (81/207)   \rReceiving objects:  40% (83/207)   \rReceiving objects:  41% (85/207)   \rReceiving objects:  42% (87/207)   \rReceiving objects:  43% (90/207)   \rReceiving objects:  44% (92/207)   \rReceiving objects:  45% (94/207)   \rReceiving objects:  46% (96/207)   \rReceiving objects:  47% (98/207)   \rReceiving objects:  48% (100/207)   \rReceiving objects:  49% (102/207)   \rReceiving objects:  50% (104/207)   \rReceiving objects:  51% (106/207)   \rReceiving objects:  52% (108/207)   \rReceiving objects:  53% (110/207)   \rReceiving objects:  54% (112/207)   \rReceiving objects:  55% (114/207)   \rReceiving objects:  56% (116/207)   \rReceiving objects:  57% (118/207)   \rReceiving objects:  58% (121/207)   \rReceiving objects:  59% (123/207)   \rReceiving objects:  60% (125/207)   \rReceiving objects:  61% (127/207)   \rReceiving objects:  62% (129/207)   \rReceiving objects:  63% (131/207)   \rReceiving objects:  64% (133/207)   \rReceiving objects:  65% (135/207)   \rReceiving objects:  66% (137/207)   \rReceiving objects:  67% (139/207)   \rReceiving objects:  68% (141/207)   \rReceiving objects:  69% (143/207)   \rReceiving objects:  70% (145/207)   \rReceiving objects:  71% (147/207)   \rReceiving objects:  72% (150/207)   \rReceiving objects:  73% (152/207)   \rReceiving objects:  74% (154/207)   \rReceiving objects:  75% (156/207)   \rReceiving objects:  76% (158/207)   \rReceiving objects:  77% (160/207)   \rReceiving objects:  78% (162/207)   \rReceiving objects:  79% (164/207)   \rReceiving objects:  80% (166/207)   \rReceiving objects:  81% (168/207)   \rReceiving objects:  82% (170/207)   \rReceiving objects:  83% (172/207)   \rReceiving objects:  84% (174/207)   \rReceiving objects:  85% (176/207)   \rReceiving objects:  86% (179/207)   \rReceiving objects:  87% (181/207)   \rReceiving objects:  88% (183/207)   \rReceiving objects:  89% (185/207)   \rReceiving objects:  90% (187/207)   \rReceiving objects:  91% (189/207)   \rremote: Total 207 (delta 37), reused 122 (delta 27), pack-reused 69\u001b[K\n",
            "Receiving objects:  92% (191/207)   \rReceiving objects:  93% (193/207)   \rReceiving objects:  94% (195/207)   \rReceiving objects:  95% (197/207)   \rReceiving objects:  96% (199/207)   \rReceiving objects:  97% (201/207)   \rReceiving objects:  98% (203/207)   \rReceiving objects:  99% (205/207)   \rReceiving objects: 100% (207/207)   \rReceiving objects: 100% (207/207), 308.44 KiB | 1.16 MiB/s, done.\n",
            "Resolving deltas:   0% (0/41)   \rResolving deltas:   2% (1/41)   \rResolving deltas:   4% (2/41)   \rResolving deltas:   7% (3/41)   \rResolving deltas:   9% (4/41)   \rResolving deltas:  21% (9/41)   \rResolving deltas:  26% (11/41)   \rResolving deltas:  29% (12/41)   \rResolving deltas:  31% (13/41)   \rResolving deltas:  34% (14/41)   \rResolving deltas:  36% (15/41)   \rResolving deltas:  39% (16/41)   \rResolving deltas:  41% (17/41)   \rResolving deltas:  43% (18/41)   \rResolving deltas:  48% (20/41)   \rResolving deltas:  51% (21/41)   \rResolving deltas:  53% (22/41)   \rResolving deltas:  58% (24/41)   \rResolving deltas:  60% (25/41)   \rResolving deltas:  65% (27/41)   \rResolving deltas:  68% (28/41)   \rResolving deltas:  70% (29/41)   \rResolving deltas:  80% (33/41)   \rResolving deltas:  85% (35/41)   \rResolving deltas:  87% (36/41)   \rResolving deltas:  97% (40/41)   \rResolving deltas: 100% (41/41)   \rResolving deltas: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-hGxdZ1aIbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r '/content/mount/My Drive/00_work/train_dt' /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA1XdwQCkAUz",
        "colab_type": "code",
        "outputId": "ecaef7b5-ea29-48fc-9d64-93b2d9a9c5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "source": [
        "!pip install tf-nightly-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/87/a3424ee94edbad9b9b5ceea0c788adf04436ed8a0cfb410b2630e0ddfc35/tf_nightly_gpu-2.3.0.dev20200520-cp36-cp36m-manylinux2010_x86_64.whl (523.3MB)\n",
            "\u001b[K     |████████████████████████████████| 523.3MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.10.0)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/e2/056033da3d84a81de310ece6d2fc1459cc42a04cdd42961a593805462b26/tb_nightly-2.3.0a20200520-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.18.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.29.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ff/c6024e54c3749b7e437acbd6a7d828ba257d20cdf998c675f4af2f671a1d/tf_estimator_nightly-2.3.0.dev2020052001-py2.py3-none-any.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly-gpu) (46.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.6.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly-gpu) (3.1.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed tb-nightly-2.3.0a20200520 tf-estimator-nightly-2.3.0.dev2020052001 tf-nightly-gpu-2.3.0.dev20200520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF3JDgMEiMyZ",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcm6d9DcBnt",
        "colab_type": "code",
        "outputId": "cf75d701-3120-4bee-f806-7f73e86ad834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/transformer\n",
        "PARAM_SET='base'\n",
        "HOME='/content/00_work'\n",
        "DATA_DIR='/content/train_dt'\n",
        "MODEL_DIR='/content/mount/My\\ Drive/00_work/model_base'\n",
        "VOCAB_FILE=DATA_DIR+'/vocab.ende.32768'\n",
        "!python transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \\\n",
        "    --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET \\\n",
        "    --train_steps=100000 --steps_between_evals=4000 \\\n",
        "    --batch_size=8192 --max_length=64 \\\n",
        "    --bleu_source=$DATA_DIR/valu_ja.csv \\\n",
        "    --bleu_ref=$DATA_DIR/valu_en.csv \\\n",
        "    --enable_time_history=false"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformer\n",
            "2020-05-21 03:29:56.692765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-21 03:30:00.358257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-05-21 03:30:00.414824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:00.415792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-05-21 03:30:00.415848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-21 03:30:00.678420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-05-21 03:30:00.794910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-05-21 03:30:00.831795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-05-21 03:30:01.087512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-05-21 03:30:01.141404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-05-21 03:30:01.652347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-05-21 03:30:01.652564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.653682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.654546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0\n",
            "2020-05-21 03:30:01.655418: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-05-21 03:30:01.661241: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-05-21 03:30:01.661516: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c1ea00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-05-21 03:30:01.661551: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-05-21 03:30:01.783849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.784884: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c1ebc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-05-21 03:30:01.784921: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-05-21 03:30:01.786237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.787135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-05-21 03:30:01.787189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-21 03:30:01.787240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-05-21 03:30:01.787276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-05-21 03:30:01.787320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-05-21 03:30:01.787356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-05-21 03:30:01.787396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-05-21 03:30:01.787432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-05-21 03:30:01.787544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.788535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:01.789387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0\n",
            "2020-05-21 03:30:01.789482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-05-21 03:30:02.457099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-05-21 03:30:02.457160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 \n",
            "2020-05-21 03:30:02.457179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N \n",
            "2020-05-21 03:30:02.457407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:02.458453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-05-21 03:30:02.459270: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-05-21 03:30:02.459321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0521 03:30:02.461441 140024468219776 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0521 03:30:02.462468 140024468219776 transformer_main.py:190] Running transformer with num_gpus = 1\n",
            "I0521 03:30:02.462875 140024468219776 transformer_main.py:194] For training, using distribution strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f598a745080>\n",
            "I0521 03:30:14.194373 140024468219776 transformer_main.py:227] Loaded checkpoint /content/mount/My Drive/00_work/model_base/cp-0002.ckpt\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "targets (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_v2 (Transformer)    (None, None, 55993)  72772096    inputs[0][0]                     \n",
            "                                                                 targets[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "logits (Lambda)                 (None, None, 55993)  0           transformer_v2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(3,)]               0           logits[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           targets[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Maximum (TensorFlow [()]                 0           tf_op_layer_strided_slice[0][0]  \n",
            "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlowOp [()]                 0           tf_op_layer_Maximum[0][0]        \n",
            "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad_1/paddings/1 (T [(2,)]               0           tf_op_layer_Sub_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowOpLa [()]                 0           tf_op_layer_Maximum[0][0]        \n",
            "                                                                 tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad_1/paddings (Ten [(2, 2)]             0           tf_op_layer_Pad_1/paddings/1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad/paddings/1 (Ten [(2,)]               0           tf_op_layer_Sub[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad_1 (TensorFlowOp [(None, None)]       0           targets[0][0]                    \n",
            "                                                                 tf_op_layer_Pad_1/paddings[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad/paddings (Tenso [(3, 2)]             0           tf_op_layer_Pad/paddings/1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast (TensorFlowOpL [(None, None)]       0           tf_op_layer_Pad_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Pad (TensorFlowOpLa [(None, None, None)] 0           logits[0][0]                     \n",
            "                                                                 tf_op_layer_Pad/paddings[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_OneHot (TensorFlowO [(None, None, 55993) 0           tf_op_layer_Cast[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_3 (TensorFlow [(3,)]               0           tf_op_layer_Pad[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_4 (TensorFlow [(3,)]               0           tf_op_layer_OneHot[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Slice (TensorFlowOp [(1,)]               0           tf_op_layer_Shape_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Slice_1 (TensorFlow [(1,)]               0           tf_op_layer_Shape_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(2,)]               0           tf_op_layer_Slice[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(2,)]               0           tf_op_layer_Slice_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(None, None)]       0           tf_op_layer_Pad[0][0]            \n",
            "                                                                 tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_1 (TensorFl [(None, None)]       0           tf_op_layer_OneHot[0][0]         \n",
            "                                                                 tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_2 (TensorFlow [(3,)]               0           tf_op_layer_Pad[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_SoftmaxCrossEntropy [(None,), (None, Non 0           tf_op_layer_Reshape[0][0]        \n",
            "                                                                 tf_op_layer_Reshape_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Slice_2 (TensorFlow [(2,)]               0           tf_op_layer_Shape_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_2 (TensorFl [(None, None)]       0           tf_op_layer_SoftmaxCrossEntropyWi\n",
            "                                                                 tf_op_layer_Slice_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_NotEqual (TensorFlo [(None, None)]       0           tf_op_layer_Pad_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlowOp [(None, None)]       0           tf_op_layer_Reshape_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_1 (TensorFlowO [(None, None)]       0           tf_op_layer_NotEqual[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul (TensorFlowOpLa [(None, None)]       0           tf_op_layer_Sub_2[0][0]          \n",
            "                                                                 tf_op_layer_Cast_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum (TensorFlowOpLa [()]                 0           tf_op_layer_Mul[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum_1 (TensorFlowOp [()]                 0           tf_op_layer_Cast_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorFlow [()]                 0           tf_op_layer_Sum[0][0]            \n",
            "                                                                 tf_op_layer_Sum_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_loss (AddLoss)              ()                   0           tf_op_layer_RealDiv[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "I0521 03:30:14.725268 140024468219776 transformer_main.py:317] Start train iteration at global step:0\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 03:31:59.014795 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 03:31:59.016716 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 03:32:04.401833 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 03:32:04.403659 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "2020-05-21 03:32:07.575819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "4000/4000 [==============================] - 2233s 558ms/step - loss: 0.0636\n",
            "I0521 04:09:22.403385 140024468219776 transformer_main.py:371] Train history: {'loss': [0.06364615261554718], 'lr': [0.00017469282], 'steps': [4000.0]}\n",
            "I0521 04:09:22.403553 140024468219776 transformer_main.py:373] End train iteration at global step:4000\n",
            "I0521 04:09:26.769961 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0001.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 04:09:27.727890 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 04:09:28.027117 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 04:09:33.172609 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 04:09:33.967398 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 04:09:34.842602 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 04:09:35.671733 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 04:09:36.519830 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 04:09:37.346870 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 04:09:38.052847 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 04:09:38.751906 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 04:09:39.427494 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 04:09:40.120662 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 04:09:40.773541 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 04:09:41.435508 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 04:09:42.093000 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 04:09:42.786093 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 04:09:43.477090 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 04:09:44.133266 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 04:09:44.834460 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 04:09:45.500089 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 04:09:46.197599 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 04:09:46.864524 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 04:09:47.483408 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 04:09:48.116969 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 04:09:48.817347 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 04:09:49.512397 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 04:09:50.171096 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 04:09:50.804886 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 04:09:51.502010 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 04:09:52.193548 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 04:09:52.867049 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 04:09:53.515183 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 04:09:54.186553 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 04:09:54.848600 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 04:09:55.475741 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 04:09:56.100183 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 04:09:56.751269 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 04:09:57.417668 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 04:09:58.115789 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 04:09:58.778028 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 04:09:59.438575 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 04:10:00.101876 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 04:10:00.741044 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 04:10:01.395921 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 04:10:02.022885 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 04:10:02.705221 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 04:10:03.268146 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 04:10:03.866192 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 04:10:04.502125 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 04:10:05.139192 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 04:10:05.760378 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 04:10:06.328795 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 04:10:06.991577 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 04:10:07.597023 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 04:10:08.208855 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 04:10:08.872074 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 04:10:09.437074 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 04:10:10.035624 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 04:10:10.599442 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 04:10:11.202705 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 04:10:11.803911 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 04:10:12.409816 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 04:10:13.075292 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 04:10:13.605735 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 04:10:17.233320 140024468219776 translate.py:184] Writing to file /tmp/tmpd6uirzi7\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpd6uirzi7 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpd6uirzi7 2000\n",
            "*******************************************************************\n",
            "I0521 04:10:18.621013 140024468219776 transformer_main.py:134] Bleu score (uncased): 72.06076979637146\n",
            "I0521 04:10:18.621762 140024468219776 transformer_main.py:135] Bleu score (cased): 72.06076979637146\n",
            "I0521 04:10:18.626924 140024468219776 transformer_main.py:317] Start train iteration at global step:4000\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 04:10:18.706268 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 04:10:18.707748 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 2/2\n",
            "4000/4000 [==============================] - 2242s 561ms/step - loss: 0.0649\n",
            "I0521 04:47:42.144673 140024468219776 transformer_main.py:371] Train history: {'loss': [0.06488128006458282], 'lr': [0.00034938563], 'steps': [8000.0]}\n",
            "I0521 04:47:42.144923 140024468219776 transformer_main.py:373] End train iteration at global step:8000\n",
            "I0521 04:47:42.152081 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0002.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 04:47:43.114431 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 04:47:43.395129 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 04:47:44.123517 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 04:47:44.809149 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 04:47:45.538928 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 04:47:46.231369 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 04:47:46.922086 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 04:47:47.641065 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 04:47:48.339902 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 04:47:49.036648 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 04:47:49.690062 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 04:47:50.587982 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 04:47:51.467873 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 04:47:52.345278 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 04:47:53.180343 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 04:47:54.032196 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 04:47:54.897767 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 04:47:55.684653 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 04:47:56.555738 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 04:47:57.369601 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 04:47:58.157978 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 04:47:58.813525 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 04:47:59.479024 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 04:48:00.137597 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 04:48:00.789764 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 04:48:01.480399 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 04:48:02.179455 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 04:48:02.858247 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 04:48:03.546530 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 04:48:04.259268 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 04:48:04.960648 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 04:48:05.603147 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 04:48:06.261651 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 04:48:06.944220 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 04:48:07.561626 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 04:48:08.226846 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 04:48:08.899803 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 04:48:09.535927 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 04:48:10.233112 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 04:48:10.889318 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 04:48:11.554308 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 04:48:12.220002 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 04:48:12.846369 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 04:48:13.474995 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 04:48:14.114576 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 04:48:14.762222 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 04:48:15.339601 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 04:48:15.999224 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 04:48:16.626766 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 04:48:17.252471 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 04:48:18.609396 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 04:48:19.180417 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 04:48:19.831573 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 04:48:20.412515 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 04:48:21.011290 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 04:48:21.667508 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 04:48:22.296816 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 04:48:22.833819 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 04:48:23.443051 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 04:48:24.025689 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 04:48:24.590391 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 04:48:25.237367 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 04:48:25.864883 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 04:48:26.440719 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 04:48:26.839110 140024468219776 translate.py:184] Writing to file /tmp/tmpa6vxufot\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpa6vxufot 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpa6vxufot 2000\n",
            "*******************************************************************\n",
            "I0521 04:48:28.196999 140024468219776 transformer_main.py:134] Bleu score (uncased): 70.89282870292664\n",
            "I0521 04:48:28.197229 140024468219776 transformer_main.py:135] Bleu score (cased): 70.89282870292664\n",
            "I0521 04:48:28.203232 140024468219776 transformer_main.py:317] Start train iteration at global step:8000\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 04:48:28.289478 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 04:48:28.291387 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 3/3\n",
            "4000/4000 [==============================] - 2236s 559ms/step - loss: 0.0843\n",
            "I0521 05:25:44.878327 140024468219776 transformer_main.py:371] Train history: {'loss': [0.08425512909889221], 'lr': [0.00052407844], 'steps': [12000.0]}\n",
            "I0521 05:25:44.878562 140024468219776 transformer_main.py:373] End train iteration at global step:12000\n",
            "I0521 05:25:44.882794 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0003.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 05:25:46.049172 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 05:25:46.342851 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 05:25:47.040816 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 05:25:47.731568 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 05:25:48.397277 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 05:25:49.077825 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 05:25:49.747432 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 05:25:50.440847 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 05:25:51.174923 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 05:25:52.110305 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 05:25:52.944576 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 05:25:53.809798 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 05:25:54.818441 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 05:25:55.705478 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 05:25:56.594492 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 05:25:57.445926 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 05:25:58.276670 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 05:25:59.078286 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 05:25:59.843262 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 05:26:00.527627 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 05:26:01.198568 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 05:26:01.819515 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 05:26:02.467483 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 05:26:03.171623 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 05:26:03.833172 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 05:26:04.503729 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 05:26:05.142824 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 05:26:05.768352 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 05:26:06.423597 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 05:26:07.080519 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 05:26:07.749297 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 05:26:08.448660 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 05:26:09.109395 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 05:26:09.778545 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 05:26:10.408486 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 05:26:11.067472 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 05:26:11.723793 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 05:26:12.354598 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 05:26:12.983118 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 05:26:13.655299 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 05:26:14.308403 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 05:26:14.974625 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 05:26:15.609693 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 05:26:16.217761 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 05:26:16.882192 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 05:26:17.549977 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 05:26:18.117982 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 05:26:18.781442 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 05:26:19.349723 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 05:26:19.985083 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 05:26:22.561223 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 05:26:23.168105 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 05:26:23.797743 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 05:26:24.357668 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 05:26:24.937671 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 05:26:25.597851 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 05:26:26.137077 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 05:26:26.712745 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 05:26:27.275892 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 05:26:27.882589 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 05:26:28.539899 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 05:26:29.150298 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 05:26:29.751762 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 05:26:30.388347 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 05:26:30.787220 140024468219776 translate.py:184] Writing to file /tmp/tmpf6hqvz6r\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpf6hqvz6r 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpf6hqvz6r 2000\n",
            "*******************************************************************\n",
            "I0521 05:26:32.219431 140024468219776 transformer_main.py:134] Bleu score (uncased): 69.71893310546875\n",
            "I0521 05:26:32.219639 140024468219776 transformer_main.py:135] Bleu score (cased): 69.71893310546875\n",
            "I0521 05:26:32.225630 140024468219776 transformer_main.py:317] Start train iteration at global step:12000\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 05:26:32.302992 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0521 05:26:32.304294 140024468219776 cross_device_ops.py:439] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 4/4\n",
            "4000/4000 [==============================] - 2236s 559ms/step - loss: 0.1063\n",
            "I0521 06:03:48.789145 140024468219776 transformer_main.py:371] Train history: {'loss': [0.10628562420606613], 'lr': [0.00069877127], 'steps': [16000.0]}\n",
            "I0521 06:03:48.789344 140024468219776 transformer_main.py:373] End train iteration at global step:16000\n",
            "I0521 06:03:48.793703 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0004.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 06:03:49.829114 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 06:03:50.110019 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 06:03:50.809575 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 06:03:51.461487 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 06:03:52.167286 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 06:03:52.847536 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 06:03:53.515998 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 06:03:54.215667 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 06:03:54.877377 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 06:03:55.724599 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 06:03:56.573137 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 06:03:57.427111 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 06:03:58.263136 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 06:03:59.064009 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 06:03:59.876224 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 06:04:00.740497 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 06:04:01.651626 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 06:04:02.475302 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 06:04:03.270066 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 06:04:03.957315 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 06:04:04.635544 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 06:04:05.281843 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 06:04:05.907767 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 06:04:06.544358 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 06:04:07.215849 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 06:04:07.909742 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 06:04:08.579524 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 06:04:09.223434 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 06:04:09.880719 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 06:04:10.540092 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 06:04:11.179387 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 06:04:11.830827 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 06:04:12.477703 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 06:04:13.142817 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 06:04:13.812448 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 06:04:14.467562 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 06:04:15.099285 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 06:04:15.812777 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 06:04:16.444252 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 06:04:17.109474 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 06:04:17.742635 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 06:04:18.416339 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 06:04:19.042154 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 06:04:19.703016 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 06:04:20.341331 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 06:04:20.999276 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 06:04:21.571380 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 06:04:22.221149 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 06:04:22.797846 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 06:04:23.461238 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 06:04:24.072534 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 06:04:24.584710 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 06:04:25.176129 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 06:04:25.784180 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 06:04:26.355898 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 06:04:26.932891 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 06:04:27.561849 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 06:04:28.186603 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 06:04:28.745930 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 06:04:29.374151 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 06:04:29.949563 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 06:04:30.618355 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 06:04:31.193438 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 06:04:31.771819 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 06:04:32.156746 140024468219776 translate.py:184] Writing to file /tmp/tmp26jbk0jg\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp26jbk0jg 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp26jbk0jg 2000\n",
            "*******************************************************************\n",
            "I0521 06:04:33.583715 140024468219776 transformer_main.py:134] Bleu score (uncased): 69.42937970161438\n",
            "I0521 06:04:33.583915 140024468219776 transformer_main.py:135] Bleu score (cased): 69.42937970161438\n",
            "I0521 06:04:33.589349 140024468219776 transformer_main.py:317] Start train iteration at global step:16000\n",
            "Epoch 5/5\n",
            "4000/4000 [==============================] - 2242s 560ms/step - loss: 0.1067\n",
            "I0521 06:41:56.468199 140024468219776 transformer_main.py:371] Train history: {'loss': [0.10669732838869095], 'lr': [0.000625], 'steps': [20000.0]}\n",
            "I0521 06:41:56.468463 140024468219776 transformer_main.py:373] End train iteration at global step:20000\n",
            "I0521 06:41:56.473384 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0005.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 06:41:57.529875 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 06:41:57.823150 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 06:41:58.526906 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 06:41:59.185507 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 06:41:59.881387 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 06:42:00.581258 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 06:42:01.277957 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 06:42:02.001078 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 06:42:02.706735 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 06:42:03.576885 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 06:42:04.398506 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 06:42:05.300786 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 06:42:06.127222 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 06:42:06.956628 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 06:42:07.758259 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 06:42:08.574914 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 06:42:09.398927 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 06:42:10.252096 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 06:42:11.059962 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 06:42:11.865522 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 06:42:12.671987 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 06:42:13.311692 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 06:42:13.955394 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 06:42:14.647657 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 06:42:15.325872 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 06:42:16.004638 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 06:42:16.667586 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 06:42:17.296181 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 06:42:17.952984 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 06:42:18.660569 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 06:42:19.351711 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 06:42:20.039845 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 06:42:20.715413 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 06:42:21.400784 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 06:42:22.051601 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 06:42:22.718494 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 06:42:23.414392 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 06:42:24.128597 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 06:42:24.784524 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 06:42:25.471250 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 06:42:26.166368 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 06:42:26.862234 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 06:42:27.511506 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 06:42:28.186809 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 06:42:28.814521 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 06:42:29.490547 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 06:42:30.043867 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 06:42:30.611594 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 06:42:31.167047 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 06:42:31.798464 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 06:42:32.439611 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 06:42:33.009490 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 06:42:33.643231 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 06:42:34.319199 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 06:42:34.975976 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 06:42:35.620050 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 06:42:36.334171 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 06:42:36.944083 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 06:42:37.538636 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 06:42:38.172645 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 06:42:38.685870 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 06:42:39.334663 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 06:42:39.902678 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 06:42:40.518601 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 06:42:40.940449 140024468219776 translate.py:184] Writing to file /tmp/tmp26r9pgq1\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp26r9pgq1 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp26r9pgq1 2000\n",
            "*******************************************************************\n",
            "I0521 06:42:42.378994 140024468219776 transformer_main.py:134] Bleu score (uncased): 69.70043182373047\n",
            "I0521 06:42:42.379288 140024468219776 transformer_main.py:135] Bleu score (cased): 69.70043182373047\n",
            "I0521 06:42:42.387077 140024468219776 transformer_main.py:317] Start train iteration at global step:20000\n",
            "Epoch 6/6\n",
            "4000/4000 [==============================] - 2241s 560ms/step - loss: 0.0904\n",
            "I0521 07:20:04.380963 140024468219776 transformer_main.py:371] Train history: {'loss': [0.0903639867901802], 'lr': [0.00057054433], 'steps': [24000.0]}\n",
            "I0521 07:20:04.381186 140024468219776 transformer_main.py:373] End train iteration at global step:24000\n",
            "I0521 07:20:04.385543 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0006.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 07:20:05.443410 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 07:20:05.734568 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 07:20:06.452227 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 07:20:07.119643 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 07:20:07.788104 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 07:20:08.469103 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 07:20:09.155470 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 07:20:09.943991 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 07:20:10.810436 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 07:20:11.703810 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 07:20:12.559624 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 07:20:13.419966 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 07:20:14.271428 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 07:20:15.084952 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 07:20:15.927881 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 07:20:16.737520 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 07:20:17.630323 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 07:20:18.376130 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 07:20:19.038200 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 07:20:19.724733 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 07:20:20.401764 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 07:20:21.047078 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 07:20:21.647922 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 07:20:22.280720 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 07:20:22.947226 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 07:20:23.573095 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 07:20:24.283581 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 07:20:24.918595 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 07:20:25.551052 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 07:20:26.245047 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 07:20:26.928684 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 07:20:27.592874 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 07:20:28.237083 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 07:20:28.910641 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 07:20:29.540206 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 07:20:30.203091 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 07:20:30.873242 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 07:20:31.504666 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 07:20:32.171498 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 07:20:32.848442 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 07:20:33.507109 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 07:20:34.171668 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 07:20:34.813042 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 07:20:35.494051 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 07:20:36.130880 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 07:20:36.807069 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 07:20:37.390888 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 07:20:38.010395 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 07:20:38.554102 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 07:20:39.173309 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 07:20:39.802697 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 07:20:40.420785 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 07:20:44.049341 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 07:20:44.647102 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 07:20:45.264912 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 07:20:45.902507 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 07:20:46.505167 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 07:20:47.176526 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 07:20:47.722752 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 07:20:48.383986 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 07:20:48.957404 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 07:20:49.573115 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 07:20:50.229161 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 07:20:50.876170 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 07:20:51.293760 140024468219776 translate.py:184] Writing to file /tmp/tmpz25hduq8\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpz25hduq8 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpz25hduq8 2000\n",
            "*******************************************************************\n",
            "I0521 07:20:52.730279 140024468219776 transformer_main.py:134] Bleu score (uncased): 70.23875117301941\n",
            "I0521 07:20:52.730492 140024468219776 transformer_main.py:135] Bleu score (cased): 70.23875117301941\n",
            "I0521 07:20:52.736328 140024468219776 transformer_main.py:317] Start train iteration at global step:24000\n",
            "Epoch 7/7\n",
            "4000/4000 [==============================] - 2245s 561ms/step - loss: 0.0801\n",
            "I0521 07:58:18.346425 140024468219776 transformer_main.py:371] Train history: {'loss': [0.0801428034901619], 'lr': [0.0005282214], 'steps': [28000.0]}\n",
            "I0521 07:58:18.346721 140024468219776 transformer_main.py:373] End train iteration at global step:28000\n",
            "I0521 07:58:18.352806 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0007.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 07:58:19.402694 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 07:58:19.686619 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 07:58:20.421649 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 07:58:21.064852 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 07:58:21.724641 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 07:58:22.411824 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 07:58:23.105040 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 07:58:23.759145 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 07:58:24.459029 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 07:58:25.271921 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 07:58:26.073372 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 07:58:26.882457 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 07:58:27.658090 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 07:58:28.500802 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 07:58:29.321705 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 07:58:30.165693 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 07:58:30.976850 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 07:58:31.793540 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 07:58:32.734505 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 07:58:33.566199 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 07:58:34.526283 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 07:58:35.357450 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 07:58:36.020258 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 07:58:36.662101 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 07:58:37.380782 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 07:58:38.060009 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 07:58:38.726745 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 07:58:39.364921 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 07:58:40.034282 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 07:58:40.716086 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 07:58:41.344791 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 07:58:42.013359 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 07:58:42.681374 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 07:58:43.561703 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 07:58:44.207092 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 07:58:44.815011 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 07:58:45.506720 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 07:58:46.140897 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 07:58:46.763381 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 07:58:47.386836 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 07:58:48.061847 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 07:58:48.694823 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 07:58:49.336975 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 07:58:49.980880 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 07:58:50.613177 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 07:58:51.247273 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 07:58:51.796909 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 07:58:52.465802 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 07:58:53.046362 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 07:58:53.723515 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 07:58:54.342195 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 07:58:54.936112 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 07:59:00.341363 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 07:59:00.914472 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 07:59:01.552215 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 07:59:02.218211 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 07:59:02.811520 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 07:59:03.419441 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 07:59:03.975050 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 07:59:04.650264 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 07:59:05.282822 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 07:59:05.889019 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 07:59:06.541080 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 07:59:07.135215 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 07:59:07.531819 140024468219776 translate.py:184] Writing to file /tmp/tmpu6wscd6_\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpu6wscd6_ 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpu6wscd6_ 2000\n",
            "*******************************************************************\n",
            "I0521 07:59:08.941004 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.7287540435791\n",
            "I0521 07:59:08.941199 140024468219776 transformer_main.py:135] Bleu score (cased): 71.7287540435791\n",
            "I0521 07:59:08.947013 140024468219776 transformer_main.py:317] Start train iteration at global step:28000\n",
            "Epoch 8/8\n",
            "4000/4000 [==============================] - 2241s 560ms/step - loss: 0.0731\n",
            "I0521 08:36:30.289274 140024468219776 transformer_main.py:371] Train history: {'loss': [0.07305573672056198], 'lr': [0.00049410586], 'steps': [32000.0]}\n",
            "I0521 08:36:30.289460 140024468219776 transformer_main.py:373] End train iteration at global step:32000\n",
            "I0521 08:36:30.300292 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0008.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 08:36:31.368618 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 08:36:31.663800 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 08:36:32.369840 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 08:36:33.043218 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 08:36:33.714589 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 08:36:34.401295 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 08:36:35.063466 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 08:36:35.737797 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 08:36:36.445228 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 08:36:37.279740 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 08:36:38.069534 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 08:36:38.875522 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 08:36:39.696862 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 08:36:40.531633 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 08:36:41.312987 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 08:36:42.189823 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 08:36:43.038441 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 08:36:43.862587 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 08:36:44.702193 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 08:36:45.573246 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 08:36:47.672878 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 08:36:48.368610 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 08:36:49.027002 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 08:36:49.678270 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 08:36:50.346379 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 08:36:50.991592 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 08:36:51.628695 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 08:36:52.265756 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 08:36:52.960501 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 08:36:53.629549 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 08:36:54.305768 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 08:36:54.989615 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 08:36:55.589442 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 08:36:56.274982 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 08:36:56.946474 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 08:36:57.595065 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 08:36:58.255567 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 08:36:58.909659 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 08:36:59.544986 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 08:37:00.229613 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 08:37:00.913670 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 08:37:01.602726 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 08:37:02.266177 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 08:37:02.871419 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 08:37:03.503102 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 08:37:04.183449 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 08:37:04.724254 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 08:37:05.338893 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 08:37:05.964832 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 08:37:06.577804 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 08:37:07.243745 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 08:37:07.760252 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 08:37:08.382953 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 08:37:08.990869 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 08:37:09.600127 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 08:37:10.260729 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 08:37:10.794816 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 08:37:11.384053 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 08:37:12.006996 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 08:37:12.640425 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 08:37:13.233487 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 08:37:13.845423 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 08:37:14.410814 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 08:37:14.953711 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 08:37:15.404857 140024468219776 translate.py:184] Writing to file /tmp/tmp87af2fx5\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp87af2fx5 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp87af2fx5 2000\n",
            "*******************************************************************\n",
            "I0521 08:37:16.807393 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.5297281742096\n",
            "I0521 08:37:16.807664 140024468219776 transformer_main.py:135] Bleu score (cased): 71.5297281742096\n",
            "I0521 08:37:16.813731 140024468219776 transformer_main.py:317] Start train iteration at global step:32000\n",
            "Epoch 9/9\n",
            "4000/4000 [==============================] - 2236s 559ms/step - loss: 0.0680\n",
            "I0521 09:14:34.210449 140024468219776 transformer_main.py:371] Train history: {'loss': [0.06799234449863434], 'lr': [0.0004658475], 'steps': [36000.0]}\n",
            "I0521 09:14:34.210650 140024468219776 transformer_main.py:373] End train iteration at global step:36000\n",
            "I0521 09:14:34.214891 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0009.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 09:14:35.219778 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 09:14:35.494629 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 09:14:36.226055 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 09:14:36.906285 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 09:14:37.629920 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 09:14:38.307816 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 09:14:38.965487 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 09:14:39.675930 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 09:14:40.499530 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 09:14:41.412436 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 09:14:42.285858 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 09:14:43.100553 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 09:14:43.933026 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 09:14:44.747557 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 09:14:45.574446 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 09:14:46.509982 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 09:14:47.389784 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 09:14:48.187158 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 09:14:48.924117 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 09:14:49.589068 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 09:14:50.293305 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 09:14:50.912291 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 09:14:51.569302 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 09:14:52.247865 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 09:14:52.901047 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 09:14:53.566677 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 09:14:54.269089 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 09:14:54.891725 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 09:14:55.586740 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 09:14:56.286891 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 09:14:56.941454 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 09:14:57.615832 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 09:14:58.279994 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 09:14:58.939320 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 09:14:59.604499 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 09:15:00.231703 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 09:15:00.936352 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 09:15:01.604764 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 09:15:02.230772 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 09:15:02.887168 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 09:15:03.573269 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 09:15:04.256915 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 09:15:04.902274 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 09:15:05.543651 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 09:15:06.174060 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 09:15:06.860452 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 09:15:07.487502 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 09:15:08.118794 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 09:15:08.698802 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 09:15:09.298710 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 09:15:09.933032 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 09:15:10.467420 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 09:15:11.132838 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 09:15:11.751669 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 09:15:12.375311 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 09:15:13.007064 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 09:15:13.649673 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 09:15:14.215584 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 09:15:14.733485 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 09:15:15.397626 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 09:15:15.997907 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 09:15:16.628385 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 09:15:17.263173 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 09:15:17.927499 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 09:15:18.334219 140024468219776 translate.py:184] Writing to file /tmp/tmpp6timr0m\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpp6timr0m 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpp6timr0m 2000\n",
            "*******************************************************************\n",
            "I0521 09:15:19.740361 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.01430892944336\n",
            "I0521 09:15:19.740778 140024468219776 transformer_main.py:135] Bleu score (cased): 71.01430892944336\n",
            "I0521 09:15:19.747504 140024468219776 transformer_main.py:317] Start train iteration at global step:36000\n",
            "Epoch 10/10\n",
            "4000/4000 [==============================] - 2241s 560ms/step - loss: 0.0642\n",
            "I0521 09:52:41.294725 140024468219776 transformer_main.py:371] Train history: {'loss': [0.06417619436979294], 'lr': [0.00044194172], 'steps': [40000.0]}\n",
            "I0521 09:52:41.294910 140024468219776 transformer_main.py:373] End train iteration at global step:40000\n",
            "I0521 09:52:41.300219 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0010.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 09:52:42.319472 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 09:52:42.611460 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 09:52:43.350031 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 09:52:44.055921 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 09:52:44.758173 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 09:52:45.421046 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 09:52:46.132647 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 09:52:46.851652 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 09:52:47.589546 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 09:52:48.467444 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 09:52:49.301121 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 09:52:50.101598 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 09:52:50.934381 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 09:52:51.740039 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 09:52:52.520779 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 09:52:53.371291 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 09:52:54.237247 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 09:52:55.040555 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 09:52:55.855226 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 09:52:56.700238 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 09:52:57.386714 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 09:52:58.057992 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 09:52:58.684596 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 09:52:59.356675 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 09:53:00.053198 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 09:53:00.727761 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 09:53:01.382670 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 09:53:02.092596 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 09:53:02.748255 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 09:53:03.438149 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 09:53:04.102425 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 09:53:04.780653 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 09:53:05.465144 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 09:53:06.136605 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 09:53:06.767365 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 09:53:07.409687 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 09:53:08.105808 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 09:53:08.785262 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 09:53:09.446101 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 09:53:10.148864 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 09:53:10.807905 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 09:53:11.474675 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 09:53:12.118811 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 09:53:12.808781 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 09:53:13.474633 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 09:53:14.145711 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 09:53:14.719835 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 09:53:15.354855 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 09:53:15.988848 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 09:53:16.593721 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 09:53:17.231021 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 09:53:17.796163 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 09:53:18.457806 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 09:53:19.059918 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 09:53:19.727551 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 09:53:20.361228 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 09:53:20.998169 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 09:53:21.570490 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 09:53:22.132416 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 09:53:22.772589 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 09:53:23.341930 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 09:53:23.982229 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 09:53:24.618233 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 09:53:25.210176 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 09:53:25.635694 140024468219776 translate.py:184] Writing to file /tmp/tmpwowwz1kz\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpwowwz1kz 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpwowwz1kz 2000\n",
            "*******************************************************************\n",
            "I0521 09:53:27.007676 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.39947414398193\n",
            "I0521 09:53:27.007926 140024468219776 transformer_main.py:135] Bleu score (cased): 71.39947414398193\n",
            "I0521 09:53:27.014027 140024468219776 transformer_main.py:317] Start train iteration at global step:40000\n",
            "Epoch 11/11\n",
            "4000/4000 [==============================] - 2240s 560ms/step - loss: 0.0611\n",
            "I0521 10:30:48.215071 140024468219776 transformer_main.py:371] Train history: {'loss': [0.06109590455889702], 'lr': [0.00042137492], 'steps': [44000.0]}\n",
            "I0521 10:30:48.215258 140024468219776 transformer_main.py:373] End train iteration at global step:44000\n",
            "I0521 10:30:48.219860 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0011.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 10:30:49.289818 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 10:30:49.580158 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 10:30:50.308368 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 10:30:50.973010 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 10:30:51.664129 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 10:30:52.377289 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 10:30:53.066667 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 10:30:53.726156 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 10:30:54.452078 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 10:30:55.264660 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 10:30:56.083588 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 10:30:56.908221 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 10:30:57.745678 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 10:30:58.515962 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 10:30:59.294440 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 10:31:00.168414 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 10:31:00.993507 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 10:31:01.791928 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 10:31:02.584393 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 10:31:03.371712 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 10:31:04.174322 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 10:31:04.844083 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 10:31:05.475436 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 10:31:06.158806 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 10:31:06.866136 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 10:31:07.526239 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 10:31:08.185622 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 10:31:08.793720 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 10:31:09.457310 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 10:31:10.153207 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 10:31:10.791388 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 10:31:11.458565 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 10:31:12.128884 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 10:31:12.795976 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 10:31:13.425533 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 10:31:14.056188 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 10:31:14.750241 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 10:31:15.415238 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 10:31:16.084346 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 10:31:16.742310 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 10:31:17.406177 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 10:31:18.072040 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 10:31:18.703610 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 10:31:19.340027 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 10:31:20.002805 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 10:31:20.658768 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 10:31:21.250526 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 10:31:21.913379 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 10:31:22.451817 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 10:31:23.057620 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 10:31:23.619661 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 10:31:24.163288 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 10:31:24.785678 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 10:31:25.367722 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 10:31:26.016971 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 10:31:26.683298 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 10:31:27.303898 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 10:31:27.868151 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 10:31:28.415192 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 10:31:29.036385 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 10:31:29.613329 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 10:31:30.256211 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 10:31:30.827419 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 10:31:31.392435 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 10:31:31.841277 140024468219776 translate.py:184] Writing to file /tmp/tmpkhk1mf3b\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpkhk1mf3b 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpkhk1mf3b 2000\n",
            "*******************************************************************\n",
            "I0521 10:31:33.194722 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.70628905296326\n",
            "I0521 10:31:33.194973 140024468219776 transformer_main.py:135] Bleu score (cased): 71.70628905296326\n",
            "I0521 10:31:33.201285 140024468219776 transformer_main.py:317] Start train iteration at global step:44000\n",
            "Epoch 12/12\n",
            "4000/4000 [==============================] - 2240s 560ms/step - loss: 0.0586\n",
            "I0521 11:08:54.629219 140024468219776 transformer_main.py:371] Train history: {'loss': [0.058579638600349426], 'lr': [0.00040343576], 'steps': [48000.0]}\n",
            "I0521 11:08:54.629440 140024468219776 transformer_main.py:373] End train iteration at global step:48000\n",
            "I0521 11:08:54.641754 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0012.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 11:08:55.669919 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 11:08:55.935390 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 11:08:56.669239 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 11:08:57.342601 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 11:08:58.000665 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 11:08:58.670841 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 11:08:59.403466 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 11:09:00.098983 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 11:09:00.843152 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 11:09:01.665447 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 11:09:02.506541 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 11:09:03.360820 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 11:09:04.213558 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 11:09:05.020693 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 11:09:05.882578 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 11:09:06.730833 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 11:09:07.625762 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 11:09:08.422433 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 11:09:09.284231 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 11:09:10.092132 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 11:09:10.787530 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 11:09:11.434491 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 11:09:12.099165 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 11:09:12.763393 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 11:09:13.475519 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 11:09:14.148628 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 11:09:14.823987 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 11:09:15.491626 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 11:09:16.195345 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 11:09:16.877600 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 11:09:17.509095 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 11:09:18.179793 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 11:09:18.828170 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 11:09:19.497556 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 11:09:20.165300 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 11:09:20.861847 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 11:09:21.564036 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 11:09:22.227838 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 11:09:22.889547 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 11:09:23.558813 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 11:09:24.199760 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 11:09:24.856709 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 11:09:25.487997 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 11:09:26.153109 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 11:09:26.793238 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 11:09:27.466506 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 11:09:28.059580 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 11:09:28.699397 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 11:09:29.406193 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 11:09:30.013540 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 11:09:30.647384 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 11:09:31.256417 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 11:09:31.891325 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 11:09:32.502917 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 11:09:33.146343 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 11:09:33.820057 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 11:09:34.516698 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 11:09:35.142203 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 11:09:35.717291 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 11:09:36.373876 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 11:09:36.943461 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 11:09:37.603270 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 11:09:38.231113 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 11:09:38.927101 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 11:09:39.357269 140024468219776 translate.py:184] Writing to file /tmp/tmp9bb0z992\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp9bb0z992 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp9bb0z992 2000\n",
            "*******************************************************************\n",
            "I0521 11:09:40.716033 140024468219776 transformer_main.py:134] Bleu score (uncased): 71.70878052711487\n",
            "I0521 11:09:40.716224 140024468219776 transformer_main.py:135] Bleu score (cased): 71.70878052711487\n",
            "I0521 11:09:40.722267 140024468219776 transformer_main.py:317] Start train iteration at global step:48000\n",
            "Epoch 13/13\n",
            "4000/4000 [==============================] - 2243s 561ms/step - loss: 0.0569\n",
            "I0521 11:47:05.028568 140024468219776 transformer_main.py:371] Train history: {'loss': [0.0568697415292263], 'lr': [0.00038760854], 'steps': [52000.0]}\n",
            "I0521 11:47:05.028770 140024468219776 transformer_main.py:373] End train iteration at global step:52000\n",
            "I0521 11:47:05.033627 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0013.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 11:47:06.092199 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 11:47:06.379198 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 11:47:07.125140 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 11:47:07.803160 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 11:47:08.530045 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 11:47:09.202767 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 11:47:09.937480 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 11:47:10.885523 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 11:47:11.710741 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 11:47:12.561610 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 11:47:13.408817 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 11:47:14.258206 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 11:47:15.091092 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 11:47:15.978234 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 11:47:16.823377 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 11:47:17.693921 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 11:47:18.585699 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 11:47:19.440716 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 11:47:20.264789 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 11:47:20.946210 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 11:47:21.650032 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 11:47:22.320235 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 11:47:22.965885 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 11:47:23.634692 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 11:47:24.315734 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 11:47:24.990924 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 11:47:25.658751 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 11:47:26.316861 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 11:47:27.034306 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 11:47:27.761275 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 11:47:28.448743 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 11:47:29.152760 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 11:47:29.832822 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 11:47:30.495847 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 11:47:31.156456 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 11:47:31.852309 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 11:47:32.540263 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 11:47:33.232343 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 11:47:33.937527 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 11:47:34.602573 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 11:47:35.265200 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 11:47:35.985543 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 11:47:36.671232 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 11:47:37.330060 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 11:47:38.000360 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 11:47:38.663217 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 11:47:39.261895 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 11:47:39.935316 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 11:47:40.585307 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 11:47:41.194796 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 11:47:41.847643 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 11:47:42.468296 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 11:47:43.160727 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 11:47:43.751583 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 11:47:44.407301 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 11:47:45.112325 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 11:47:45.757277 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 11:47:46.452888 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 11:47:47.069344 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 11:47:47.777815 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 11:47:48.341227 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 11:47:48.999273 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 11:47:49.593016 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 11:47:50.335263 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 11:47:50.761492 140024468219776 translate.py:184] Writing to file /tmp/tmp67925me_\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp67925me_ 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmp67925me_ 2000\n",
            "*******************************************************************\n",
            "I0521 11:47:52.166121 140024468219776 transformer_main.py:134] Bleu score (uncased): 72.03812003135681\n",
            "I0521 11:47:52.166451 140024468219776 transformer_main.py:135] Bleu score (cased): 72.03812003135681\n",
            "I0521 11:47:52.176293 140024468219776 transformer_main.py:317] Start train iteration at global step:52000\n",
            "Epoch 14/14\n",
            "4000/4000 [==============================] - 2233s 558ms/step - loss: 0.0554\n",
            "I0521 12:25:06.543855 140024468219776 transformer_main.py:371] Train history: {'loss': [0.055356692522764206], 'lr': [0.00037350893], 'steps': [56000.0]}\n",
            "I0521 12:25:06.544110 140024468219776 transformer_main.py:373] End train iteration at global step:56000\n",
            "I0521 12:25:06.548782 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0014.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 12:25:07.576018 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 12:25:07.863241 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 12:25:08.561012 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 12:25:09.236099 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 12:25:09.923865 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 12:25:10.641654 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 12:25:11.634554 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 12:25:12.435076 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 12:25:13.362745 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 12:25:14.250699 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 12:25:15.097766 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 12:25:15.999350 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 12:25:16.851404 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 12:25:17.658844 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 12:25:18.499265 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 12:25:19.351403 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 12:25:20.170651 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 12:25:20.843737 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 12:25:21.557866 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 12:25:22.227004 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 12:25:22.924890 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 12:25:23.597275 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 12:25:24.232438 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 12:25:24.902792 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 12:25:25.613901 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 12:25:26.279614 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 12:25:26.958557 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 12:25:27.618184 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 12:25:28.312685 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 12:25:29.021196 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 12:25:29.716498 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 12:25:30.392633 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 12:25:31.069872 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 12:25:31.741525 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 12:25:32.431843 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 12:25:33.125299 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 12:25:33.889814 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 12:25:34.512577 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 12:25:35.209013 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 12:25:35.877426 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 12:25:36.533146 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 12:25:37.215701 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 12:25:37.881730 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 12:25:38.550518 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 12:25:39.195430 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 12:25:39.861917 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 12:25:40.432663 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 12:25:41.041384 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 12:25:41.606542 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 12:25:42.223810 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 12:25:42.854457 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 12:25:43.471352 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 12:25:44.150277 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 12:25:44.779457 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 12:25:45.439827 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 12:25:46.083326 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 12:25:46.767425 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 12:25:47.386085 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 12:25:47.960474 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 12:25:48.669924 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 12:25:49.257115 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 12:25:49.883589 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 12:25:50.561416 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 12:25:51.175758 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 12:25:51.655070 140024468219776 translate.py:184] Writing to file /tmp/tmpu4aqw7tj\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpu4aqw7tj 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpu4aqw7tj 2000\n",
            "*******************************************************************\n",
            "I0521 12:25:53.066454 140024468219776 transformer_main.py:134] Bleu score (uncased): 72.21116423606873\n",
            "I0521 12:25:53.066766 140024468219776 transformer_main.py:135] Bleu score (cased): 72.21116423606873\n",
            "I0521 12:25:53.073738 140024468219776 transformer_main.py:317] Start train iteration at global step:56000\n",
            "Epoch 15/15\n",
            "4000/4000 [==============================] - 2244s 561ms/step - loss: 0.0535\n",
            "I0521 13:03:18.717381 140024468219776 transformer_main.py:371] Train history: {'loss': [0.053499091416597366], 'lr': [0.00036084393], 'steps': [60000.0]}\n",
            "I0521 13:03:18.717570 140024468219776 transformer_main.py:373] End train iteration at global step:60000\n",
            "I0521 13:03:18.722872 140024468219776 transformer_main.py:448] Load weights: /content/mount/My Drive/00_work/model_base/cp-0015.ckpt\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "transformer_v2 (Transformer) {'outputs': (None, None), 72772096  \n",
            "=================================================================\n",
            "Total params: 72,772,096\n",
            "Trainable params: 72,772,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "I0521 13:03:19.823346 140024468219776 tokenizer.py:75] Initializing Subtokenizer from file /content/train_dt/vocab.ende.32768.\n",
            "I0521 13:03:20.124252 140024468219776 translate.py:122] Decoding batch 0 out of 63.\n",
            "I0521 13:03:20.879716 140024468219776 translate.py:122] Decoding batch 1 out of 63.\n",
            "I0521 13:03:21.590202 140024468219776 translate.py:122] Decoding batch 2 out of 63.\n",
            "I0521 13:03:22.304560 140024468219776 translate.py:122] Decoding batch 3 out of 63.\n",
            "I0521 13:03:22.994140 140024468219776 translate.py:122] Decoding batch 4 out of 63.\n",
            "I0521 13:03:23.696019 140024468219776 translate.py:122] Decoding batch 5 out of 63.\n",
            "I0521 13:03:24.445086 140024468219776 translate.py:122] Decoding batch 6 out of 63.\n",
            "I0521 13:03:25.335169 140024468219776 translate.py:122] Decoding batch 7 out of 63.\n",
            "I0521 13:03:26.207220 140024468219776 translate.py:122] Decoding batch 8 out of 63.\n",
            "I0521 13:03:27.123030 140024468219776 translate.py:122] Decoding batch 9 out of 63.\n",
            "I0521 13:03:28.053433 140024468219776 translate.py:122] Decoding batch 10 out of 63.\n",
            "I0521 13:03:28.870052 140024468219776 translate.py:122] Decoding batch 11 out of 63.\n",
            "I0521 13:03:29.734692 140024468219776 translate.py:122] Decoding batch 12 out of 63.\n",
            "I0521 13:03:30.574912 140024468219776 translate.py:122] Decoding batch 13 out of 63.\n",
            "I0521 13:03:31.451661 140024468219776 translate.py:122] Decoding batch 14 out of 63.\n",
            "I0521 13:03:32.363005 140024468219776 translate.py:122] Decoding batch 15 out of 63.\n",
            "I0521 13:03:33.236126 140024468219776 translate.py:122] Decoding batch 16 out of 63.\n",
            "I0521 13:03:33.917319 140024468219776 translate.py:122] Decoding batch 17 out of 63.\n",
            "I0521 13:03:34.615810 140024468219776 translate.py:122] Decoding batch 18 out of 63.\n",
            "I0521 13:03:35.309921 140024468219776 translate.py:122] Decoding batch 19 out of 63.\n",
            "I0521 13:03:36.007646 140024468219776 translate.py:122] Decoding batch 20 out of 63.\n",
            "I0521 13:03:36.680443 140024468219776 translate.py:122] Decoding batch 21 out of 63.\n",
            "I0521 13:03:37.368276 140024468219776 translate.py:122] Decoding batch 22 out of 63.\n",
            "I0521 13:03:38.090801 140024468219776 translate.py:122] Decoding batch 23 out of 63.\n",
            "I0521 13:03:38.802309 140024468219776 translate.py:122] Decoding batch 24 out of 63.\n",
            "I0521 13:03:39.500153 140024468219776 translate.py:122] Decoding batch 25 out of 63.\n",
            "I0521 13:03:40.169422 140024468219776 translate.py:122] Decoding batch 26 out of 63.\n",
            "I0521 13:03:40.873418 140024468219776 translate.py:122] Decoding batch 27 out of 63.\n",
            "I0521 13:03:41.623979 140024468219776 translate.py:122] Decoding batch 28 out of 63.\n",
            "I0521 13:03:42.335308 140024468219776 translate.py:122] Decoding batch 29 out of 63.\n",
            "I0521 13:03:43.032772 140024468219776 translate.py:122] Decoding batch 30 out of 63.\n",
            "I0521 13:03:43.714820 140024468219776 translate.py:122] Decoding batch 31 out of 63.\n",
            "I0521 13:03:44.405578 140024468219776 translate.py:122] Decoding batch 32 out of 63.\n",
            "I0521 13:03:45.105701 140024468219776 translate.py:122] Decoding batch 33 out of 63.\n",
            "I0521 13:03:45.816822 140024468219776 translate.py:122] Decoding batch 34 out of 63.\n",
            "I0521 13:03:46.514413 140024468219776 translate.py:122] Decoding batch 35 out of 63.\n",
            "I0521 13:03:47.234206 140024468219776 translate.py:122] Decoding batch 36 out of 63.\n",
            "I0521 13:03:47.957290 140024468219776 translate.py:122] Decoding batch 37 out of 63.\n",
            "I0521 13:03:48.635187 140024468219776 translate.py:122] Decoding batch 38 out of 63.\n",
            "I0521 13:03:49.307073 140024468219776 translate.py:122] Decoding batch 39 out of 63.\n",
            "I0521 13:03:50.028883 140024468219776 translate.py:122] Decoding batch 40 out of 63.\n",
            "I0521 13:03:50.725025 140024468219776 translate.py:122] Decoding batch 41 out of 63.\n",
            "I0521 13:03:51.367550 140024468219776 translate.py:122] Decoding batch 42 out of 63.\n",
            "I0521 13:03:52.052357 140024468219776 translate.py:122] Decoding batch 43 out of 63.\n",
            "I0521 13:03:52.733654 140024468219776 translate.py:122] Decoding batch 44 out of 63.\n",
            "I0521 13:03:53.400529 140024468219776 translate.py:122] Decoding batch 45 out of 63.\n",
            "I0521 13:03:54.047306 140024468219776 translate.py:122] Decoding batch 46 out of 63.\n",
            "I0521 13:03:54.667924 140024468219776 translate.py:122] Decoding batch 47 out of 63.\n",
            "I0521 13:03:55.289528 140024468219776 translate.py:122] Decoding batch 48 out of 63.\n",
            "I0521 13:03:55.937681 140024468219776 translate.py:122] Decoding batch 49 out of 63.\n",
            "I0521 13:03:56.556204 140024468219776 translate.py:122] Decoding batch 50 out of 63.\n",
            "I0521 13:03:57.240416 140024468219776 translate.py:122] Decoding batch 51 out of 63.\n",
            "I0521 13:03:57.782810 140024468219776 translate.py:122] Decoding batch 52 out of 63.\n",
            "I0521 13:03:58.480697 140024468219776 translate.py:122] Decoding batch 53 out of 63.\n",
            "I0521 13:03:59.154237 140024468219776 translate.py:122] Decoding batch 54 out of 63.\n",
            "I0521 13:03:59.800982 140024468219776 translate.py:122] Decoding batch 55 out of 63.\n",
            "I0521 13:04:00.394631 140024468219776 translate.py:122] Decoding batch 56 out of 63.\n",
            "I0521 13:04:00.993163 140024468219776 translate.py:122] Decoding batch 57 out of 63.\n",
            "I0521 13:04:01.570609 140024468219776 translate.py:122] Decoding batch 58 out of 63.\n",
            "I0521 13:04:02.118537 140024468219776 translate.py:122] Decoding batch 59 out of 63.\n",
            "I0521 13:04:02.753578 140024468219776 translate.py:122] Decoding batch 60 out of 63.\n",
            "I0521 13:04:03.428094 140024468219776 translate.py:122] Decoding batch 61 out of 63.\n",
            "I0521 13:04:04.047731 140024468219776 translate.py:122] Decoding batch 62 out of 63.\n",
            "I0521 13:04:04.466694 140024468219776 translate.py:184] Writing to file /tmp/tmpxae13z0n\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpxae13z0n 2000\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "/content/train_dt/valu_en.csv 2000\n",
            "/tmp/tmpxae13z0n 2000\n",
            "*******************************************************************\n",
            "I0521 13:04:05.935743 140024468219776 transformer_main.py:134] Bleu score (uncased): 72.1150815486908\n",
            "I0521 13:04:05.935975 140024468219776 transformer_main.py:135] Bleu score (cased): 72.1150815486908\n",
            "I0521 13:04:05.942433 140024468219776 transformer_main.py:317] Start train iteration at global step:60000\n",
            "Epoch 16/16\n",
            "1159/4000 [=======>......................] - ETA: 26:01 - loss: 0.0540Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2x8PcXZb4b-",
        "colab_type": "code",
        "outputId": "4ce5fee4-0b31-4243-c418-449f6266ac0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbKr8w5j8YVB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmh1E0B65w9F",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DAZa2xXlla8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -v "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hudMtvUa1Wt4",
        "colab_type": "text"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ363laVmkAt",
        "colab_type": "text"
      },
      "source": [
        "# make pb file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q91jN6BPNMGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] = '/env/python:/content/sys_oper/00_src_models/research/:/content/sys_oper/00_src_models/research/slim/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZuE5P6mYDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "PIPELINE_FNAME = '/content/sys_oper/01_configfile/ssd_resnet50_v1_fpn_shared_box_predictor_gt.config'\n",
        "MODEL_DIR = '/content/mount/My\\ Drive/00_work/ssd_ysl_ssd_resnet50_sysoper_gtgt2'\n",
        "CONFIG_FILE='/content/sys_oper/01_configfile/ssd_resnet50_v1_fpn_shared_box_predictor_gt.config'\n",
        "EXPORT_OUTPUT_DIR='/content/mount/My\\ Drive/00_work/ssd_ysl_ssd_resnet50_sysoper_gtgt2/tflite'\n",
        "CHECKPOINT_PATH='/content/mount/My\\ Drive/00_work/ssd_ysl_ssd_resnet50_sysoper_gtgt2/model.ckpt-11106'\n",
        "!python /content/sys_oper/00_src_models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "--pipeline_config_path=$CONFIG_FILE \\\n",
        "--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\n",
        "--output_directory=$EXPORT_OUTPUT_DIR \\\n",
        "--add_postprocessing_op=true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI-etnP8muxZ",
        "colab_type": "text"
      },
      "source": [
        "#make tflite file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Dv06H3misk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_MODEL='/content/mount/My\\ Drive/00_work/ssd_ysl_ssd_resnet50_sysoper_gtgt2/tflite'\n",
        "!tflite_convert    \\\n",
        " --graph_def_file={PATH_TO_MODEL}/tflite_graph.pb \\\n",
        " --output_file={PATH_TO_MODEL}/detectfpn300.tflite \\\n",
        " --input_shapes=1,800,800,3 \\\n",
        " --output_format=TFLITE \\\n",
        " --input_arrays=normalized_input_image_tensor \\\n",
        " --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
        " --inference_type=FLOAT \\\n",
        " --mean_values=128 \\\n",
        " --std_dev_values=128 \\\n",
        " --change_concat_input_ranges=false \\\n",
        " --allow_custom_ops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWWajjttWo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRnBGrVJ1qC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_directory = './fine_tuned_model'\n",
        "\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "    \n",
        "lst = os.listdir(MODEL_DIR)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps = np.array([int(re.findall('\\d+',l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "last_model_path = os.path.join(MODEL_DIR, last_model)\n",
        "print(last_model_path)\n",
        "\n",
        "!python /content/Finetune_SSD_MobileNetV2/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pou9mUiw4N_R",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFMVh4Bb12uX",
        "colab_type": "code",
        "outputId": "9310b098-a809-42aa-e10a-b8bcab7e637d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "log = os.listdir(MODEL_DIR)\n",
        "log = [l for l in log if 'events.out.tfevents.' in l]\n",
        "\n",
        "for item in log:\n",
        "    print(output_directory)\n",
        "    shutil.copy(os.path.join(MODEL_DIR, item), output_directory)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./fine_tuned_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvCXMwjG5Mg1",
        "colab_type": "code",
        "outputId": "df0e66cf-5035-4408-b3b5-fb483f9dc0b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "DRIVE_WORKSPACE = '/content/mount/My\\ Drive/DogCatDetection'\n",
        "LATEST_CHECKPOINT = '/content/mount/My\\ Drive/DogCatDetection/last_checkpoint'\n",
        "FOLDER_CHECKPOINT = '/content/mount/My\\ Drive/DogCatDetection/'+'train_' # setting number here\n",
        "# Function to back up all checkpoint: Folder train + number\n",
        "arr = []\n",
        "!ls {DRIVE_WORKSPACE}\n",
        "arr = !ls {DRIVE_WORKSPACE}\n",
        "swap = []\n",
        "\n",
        "def getFolder(arr):\n",
        "    idx = []\n",
        "    for elem in arr:\n",
        "        if elem[:5] == 'train':\n",
        "            idx.append(int(elem[6:]))\n",
        "    return idx\n",
        "\n",
        "if len(arr) == 0: \n",
        "    FOLDER_CHECKPOINT = FOLDER_CHECKPOINT + '{}'.format(1) # create 1st train directory\n",
        "    !cp -a {output_directory}/. {FOLDER_CHECKPOINT} # copy output directory to drive\n",
        "else:\n",
        "    for small_arr in arr:\n",
        "        for elem in small_arr.split():\n",
        "            swap.append(elem)\n",
        "    arr = swap\n",
        "\n",
        "    idx = getFolder(arr)\n",
        "    print(idx)   \n",
        "    print('Last trained folder: {}'.format(max(idx)))\n",
        "    FOLDER_CHECKPOINT = FOLDER_CHECKPOINT + '{}'.format(int(max(idx))+1) # create new train directory\n",
        "    !cp -a {output_directory}/. {FOLDER_CHECKPOINT} # copy output directory to drive\n",
        "\n",
        "arr = []\n",
        "arr = !ls {DRIVE_WORKSPACE}\n",
        "swap = []\n",
        "\n",
        "for small_arr in arr:\n",
        "    for elem in small_arr.split():\n",
        "        swap.append(elem)\n",
        "arr = swap\n",
        "idx = getFolder(arr)\n",
        "print(idx) \n",
        "print('Number of trained folder: {}'.format(len(idx)))\n",
        "print('New trained folder: {}'.format(max(idx)))\n",
        "\n",
        "!ls {DRIVE_WORKSPACE}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label_map\t test_img  train_10  train_2  train_4  train_6\ttrain_8\n",
            "last_checkpoint  train_1   train_11  train_3  train_5  train_7\ttrain_9\n",
            "[10, 2, 4, 6, 8, 1, 11, 3, 5, 7, 9]\n",
            "Last trained folder: 11\n",
            "[1, 12, 4, 7, 10, 2, 5, 8, 11, 3, 6, 9]\n",
            "Number of trained folder: 12\n",
            "New trained folder: 12\n",
            "label_map\t train_1   train_12  train_4  train_7\n",
            "last_checkpoint  train_10  train_2   train_5  train_8\n",
            "test_img\t train_11  train_3   train_6  train_9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGanRe7-5ewV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -e {LATEST_CHECKPOINT} && rm -rf {LATEST_CHECKPOINT} || echo LATEST_CHECKPOINT not found && mkdir {LATEST_CHECKPOINT} # create new ckpt\n",
        "!cd {DRIVE_WORKSPACE} && ls \n",
        "!cp -a {output_directory}/. {LATEST_CHECKPOINT}\n",
        "!cd {LATEST_CHECKPOINT} && ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QnKeImvKq9H",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-WnHS_Twx1f",
        "colab_type": "text"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh9An6nz6AB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daxkl3PXzvkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'modelInference'\n",
        "%cd 'modelInference'\n",
        "!mkdir 'test_images' && cd 'test_images' && pwd\n",
        "!cd 'test_images' && wget 'https://www.petmd.com/sites/default/files/Acute-Dog-Diarrhea-47066074.jpg'\n",
        "!cd 'test_images' && wget 'https://cdn-images-1.medium.com/max/1600/1*mONNI1lG9VuiqovpnYqicA.jpeg'\n",
        "!cd 'test_images' && wget 'https://www.lifewithdogs.tv/wp-content/uploads/2015/01/gfhdgfgfgdfgdf.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf87W3WW0_Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp {LATEST_CHECKPOINT}/frozen_inference_graph.pb ./\n",
        "!cp {PATH_TO_LABELS} ./\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OZn0azu1Vgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_FROZEN_GRAPH = os.path.join(os.path.abspath('/content/Finetune_SSD_MobileNetV2/models/research/modelInference'), \"frozen_inference_graph.pb\")\n",
        "assert os.path.isfile(PATH_TO_FROZEN_GRAPH), '`{}` not exist'.format(PATH_TO_FROZEN_GRAPH)\n",
        "print(PATH_TO_FROZEN_GRAPH)\n",
        "\n",
        "PATH_TO_LABELS = os.path.join(os.path.abspath('/content/Finetune_SSD_MobileNetV2/models/research/modelInference'), \"pet_label_map.pbtxt\")\n",
        "assert os.path.isfile(PATH_TO_LABELS), '`{}` not exist'.format(PATH_TO_LABELS)\n",
        "print(PATH_TO_LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W50OvkmL2QXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_TEST_IMAGES_DIR = '/content/Finetune_SSD_MobileNetV2/models/research/modelInference/test_images'\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIGK4qn01srX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "        \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aubCZ1UQ1_Z3",
        "colab_type": "code",
        "outputId": "2d59897d-bbdd-4c9c-fd56-233939394911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_classes = get_num_classes(PATH_TO_LABELS)\n",
        "num_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKc8-gXm2E-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcDAxLQ-2Ik3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    print(image_path)\n",
        "    print(output_dict['detection_classes'][:5])\n",
        "    print(output_dict['detection_scores'][:5])\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8xVdsEW2Yy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}